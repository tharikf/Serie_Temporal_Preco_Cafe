
## Revisão - Séries Temporais ##
	# Material Duke University - Robert Nau #

Primeiro Capítulo: Get to know your data

	In other words, figure out the way in which the future will look very much like the present, only longer.

	The variable you want to forecast can be viewed as a combination of signal and noise.

	The signal is the predictable component, and the noise is what is left over.

	The technical term for a data series that is pure noise is that it is a sequence of “independent and
	identically-distributed (i.i.d.) random variables.”

	Some of the simplest signal-noise patterns that you find in data are i.i.d. variations around a line
	or curve, or i.i.d. changes relative to an earlier period in time.

		The appropriate forecasting model for this is data series is the mean model, in which you simply
		predict that series will equal its mean value in every period. This is a special case of a regression
		model in which there is an intercept term but no independent variables, i.e., an “intercept-only”
		model.

		The mean model is not as trivial as it might first appear: you need to estimate the mean as well
		as the standard deviation of the variations around the mean, and the standard deviation needs to
		be used appropriately to calculate confidence intervals for the predictions.

	A slightly more interesting pattern is that of i.i.d. variations around a sloping line on a plot of
	your variable-of-interest versus some other variable, which indicates some degree of correlation
	between the two variables.

		The appropriate forecasting model in this case would be a simple regression model. If the X axis
		is the time index, it is called a trend line model. If you transform the variable by “de-trending,”
		i.e., subtracting out the trend line, it becomes a time series that looks like it was generated by the
		mean model.

	Another more interesting pattern is that of a time series that undergoes i.i.d. changes from one
	period to the next, which might look like this.

		The appropriate forecasting model for this series is the random walk model, so-called because
		the variable takes random steps up and down as it goes forward.

		If you transform the variable by computing its period-toperiod changes
		(the “first difference” of the time series), it becomes a time series that is described by the mean model.

		What is particularly interesting about the random walk model is
		the precise way in which the confidence limits for the forecasts get wider at longer forecast
		horizons, which is central to the theory of option pricing in finance. 

		You need to analyze the statistical properties of the steps (in particular, the correlation between
		the size of the step in one period and the sizes of steps in preceding periods) to determine if they are
		truly random or if they have non-random properties such as “momentum” or “mean-reversion” or “seasonality.”

	Another commonly seen pattern in time series data is that of i.i.d. variations around a seasonal
	pattern, which might look like this.

		This sort of pattern is seen in retail sales, air travel, tourism, housing construction, power and
		water consumption, and many other measures of economic activity and environmental conditions.

		The noise is often not immediately apparent on such a graph, because the random variations are
		usually small in comparison to the seasonal variations, but the random variations are very
		important in seasonally-adjusted terms.

	There are three distinct sources of forecasting risk and corresponding ways to measure and try to
	reduce them.

		Intrinsic risk is random variation that is beyond explanation with the data and tools
		you have available. It’s the “noise” in the system.

			The intrinsic risk is usually measured by the “standard error of the model,” which is the
			estimated standard deviation of the noise in the variable you are trying to predict.

		Parameter risk is the risk due to errors in estimating the parameters of the
		forecasting model you are using, under the assumption that you are fitting the correct
		model to the data in the first place.

			This is usually a much smaller source of forecast error than intrinsic risk,
			if the model is really correct.

			Parameter risk can be reduced in principle by obtaining a larger sample of data.
			However, when you are predicting time series, more sample data is not always better.

			Using a larger sample might mean including older data that is not as representative of current conditions.
			No pattern really stays the same forever, which is known as the “blur of history” problem.

		Model risk is the risk of choosing the wrong model, i.e., making the wrong
		assumptions about whether or how the future will resemble the past.

			This is usually the most serious form of forecast error, and there is no “standard error” for
			measuring it, because every model assumes itself to be correct.

	To the extent that you can’t reduce or eliminate intrinsic risk and parameter risk, you
	can and should try to quantify them in realistic terms, so as to be honest in your
	reporting and so that appropriate risk-return tradeoffs can be made when decisions are
	based on the forecast.

	How do you know when your model is good, or at least not obviously bad? One very basic test
	of your model is whether its errors really look like pure noise, i.e., independent and identically
	distributed random variables.

	If the errors are not pure noise, then by definition there is some
	pattern in them, and you could make them smaller by adjusting the model to explain that pattern.


Segundo Capítulo (pt.1): Introduction to forecasting: the simples models - THE SAMPLE MEAN

	How should we forecast what will happen next? The simplest forecasting model that we might consider is
	the mean model, which assumes that the time series consists of independently and identically distributed (“i.i.d.”)
	values, as if each observation is randomly drawn from the same population.

	Under this assumption, the next value should be predicted to be equal to the
	historical sample mean if the goal is to minimize mean squared error.

	It is traditional in the field of statistics to measure variability in terms of average squared deviations
	instead of average absolute deviations around a central value, because squared error has a lot of nice properties:

		The central value around which the sum of squared deviations are minimized is, in fact, the sample mean.

		From a decision-theoretic viewpoint, large errors often have disproportionately worse consequences than small errors,
		hence squared error is more representative of the economic consequences of error.

	Furthermore, nonlinear transformations of the data (e.g., log or power transformations) can often
	be used to turn skewed distributions into symmetric (ideally normal) ones, allowing such data to
	be well fitted by models that focus on mean values.

	Fundamental law of forecasting risk:

		Variance of forecasting risk = variance of intrinsic risk + variance of parameter risk

	In general, the estimated parameter risk is a relatively small component of the forecast standard
	error if (i) the number of data points is large in relation to the number of parameters estimated,
	and (ii) the model is not attempting to extrapolate trends too far into the future or otherwise
	make predictions for what will happen far away from the “center of mass” of the data that was fitted.

	A point forecast should always be accompanied by a confidence interval to indicate the accuracy that
	is claimed for it, but what does “confidence” mean? It’s sort of like “probability,” but not exactly.


Segundo Capítulo (pt.2): Introduction to forecasting: the simples models - THE RANDOM WALK















	



























































